# -*- coding: utf-8 -*-
"""Sentiment Analysis Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEP-EQC1tBeLw21-AExstOIm0D_GWp12
"""

import board
import neopixel
import sounddevice as sd
import numpy as np
import queue
import json
from vosk import Model, KaldiRecognizer
import threading
from textblob import TextBlob
import time


NUM_LEDS = 12
pixels = neopixel.NeoPixel(board.D18, NUM_LEDS, auto_write=False)


HAPPY_COLOR = (255, 255, 0)    # yellow
ANGRY_COLOR = (255, 0, 0)      # red
NEUTRAL_COLOR = (128, 0, 128)  # purple
SILENCE_COLOR = (0, 0, 255)    # blue


negative_keywords = ['fuck', 'shit', 'damn', 'bitch', 'dick', 'fucker', 'motherfucker']
happy_keywords = ['love', 'happy', 'great', 'awesome', 'fun', 'yay', 'fantastic', 'good', 'joy', 'amazing', 'miss you']


q = queue.Queue()

def audio_callback(indata, frames, time, status):
    if status:
        print(status)
    q.put(indata.copy())


model = Model("/home/anas/vosk-model/vosk-model-small-en-us-0.15")
recognizer = KaldiRecognizer(model, 44100)


current_mood = 'silence'
lock = threading.Lock()

def set_mood_color(mood):
    global current_mood
    with lock:
        current_mood = mood  # update immediately for live response


def breathing(color, speed=0.02):
    for i in range(0, 256, 5):
        scaled = tuple(int(c * i / 255) for c in color)
        pixels.fill(scaled)
        pixels.show()
        time.sleep(speed)
    for i in range(255, 0, -5):
        scaled = tuple(int(c * i / 255) for c in color)
        pixels.fill(scaled)
        pixels.show()
        time.sleep(speed)

def shooting(color, flashes=3, speed=0.05):
    for _ in range(flashes):
        pixels.fill(color)
        pixels.show()
        time.sleep(speed)
        pixels.fill((0,0,0))
        pixels.show()
        time.sleep(speed)

def pulsing(color, speed=0.01):
    for i in range(0, 256, 5):
        scaled = tuple(int(c * i / 255) for c in color)
        pixels.fill(scaled)
        pixels.show()
        time.sleep(speed)
    for i in range(255, 0, -5):
        scaled = tuple(int(c * i / 255) for c in color)
        pixels.fill(scaled)
        pixels.show()
        time.sleep(speed)


def process_audio():
    global current_mood
    while True:
        audio_chunk = q.get()
        audio_chunk = (audio_chunk.flatten() * 32768).astype(np.int16).tobytes()

        if recognizer.AcceptWaveform(audio_chunk):
            result_json = recognizer.Result()
        else:
            result_json = recognizer.PartialResult()

        result = json.loads(result_json)
        text = result.get("text", "") or result.get("partial", "")

        if text.strip() == "":
            mood = 'silence'
        else:
            text_lower = text.lower()
            if any(word in text_lower for word in negative_keywords):
                mood = 'angry'
            elif any(word in text_lower for word in happy_keywords):
                mood = 'happy'
            else:
                sentiment = TextBlob(text).sentiment.polarity
                if sentiment > 0.2:
                    mood = 'happy'
                elif sentiment < -0.2:
                    mood = 'angry'
                else:
                    mood = 'neutral'

        set_mood_color(mood)
        print("Heard:", text.strip(), "| Mood:", mood)  # text stays local to this function



def effect_loop():
    while True:
        with lock:
            mood = current_mood
        if mood == 'neutral':
            breathing(NEUTRAL_COLOR)
        elif mood == 'angry':
            shooting(ANGRY_COLOR)
        elif mood == 'happy':
            pulsing(HAPPY_COLOR)
        else:
            pixels.fill(SILENCE_COLOR)
            pixels.show()
            time.sleep(0.1)


stream = sd.InputStream(channels=1, callback=audio_callback, samplerate=44100)
threading.Thread(target=process_audio, daemon=True).start()
threading.Thread(target=effect_loop, daemon=True).start()

print("Live Mood Light Active! Speak into the mic...")

with stream:
    while True:
        sd.sleep(1000)